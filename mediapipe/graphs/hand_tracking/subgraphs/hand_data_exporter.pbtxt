# MediaPipe graph to render hand landmarks and some related debug information.

type: "HandDataExporterSubgraph"

# CPU image. (ImageFrame)
input_stream: "IMAGE:input_image"
# Collection of detected/predicted hands, each represented as a list of
# landmarks. (std::vector<NormalizedLandmarkList>)
input_stream: "LANDMARKS:multi_hand_landmarks"
# Handedness of the detected hand (i.e. is hand left or right).
# (std::vector<ClassificationList>)
input_stream: "HANDEDNESS:multi_handedness"
# Regions of interest calculated based on palm detections.
# (std::vector<NormalizedRect>)
input_stream: "NORM_RECTS:0:multi_palm_rects"
# Regions of interest calculated based on landmarks.
# (std::vector<NormalizedRect>)
input_stream: "NORM_RECTS:1:multi_hand_rects"
# Detected palms. (std::vector<Detection>)
input_stream: "DETECTIONS:palm_detections"

# Updated CPU image. (ImageFrame)
output_stream: "IMAGE:output_image"

# Converts detections json
node {
  calculator: "DetectionsToTrainDataCalculator"
  input_stream: "IMAGE:input_image"
  input_stream: "DETECTIONS:palm_detections"
  output_stream: "RENDER_DATA:detection_render_data"
  node_options: {
    [type.googleapis.com/mediapipe.DetectionsToRenderDataCalculatorOptions] {
      thickness: 4.0
      color { r: 0 g: 255 b: 0 }
    }
  }
}
# Draws annotations and overlays them on top of the input images. Consumes
# a vector of RenderData objects and draws each of them on the input frame.
node {
  calculator: "AnnotationOverlayCalculator"
  input_stream: "IMAGE:input_image"
  input_stream: "detection_render_data"
  # input_stream: "multi_hand_rects_render_data"
  # input_stream: "multi_palm_rects_render_data"
  # input_stream: "handedness_render_data"
  # input_stream: "VECTOR:0:multi_hand_landmarks_render_data"
  output_stream: "IMAGE:output_image"
}